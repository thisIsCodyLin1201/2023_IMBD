{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import pandas as pd\n","import os"],"metadata":{"id":"NBGHEvSdIDnU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount(\"/content/drive\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"C5GqD7nbQcd2","executionInfo":{"status":"ok","timestamp":1694503301454,"user_tz":-480,"elapsed":18385,"user":{"displayName":"林耘熙","userId":"06114259024859577512"}},"outputId":"12c9dad6-b68a-41b8-ffe6-cd49665fcd74"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","source":["#敘述統計"],"metadata":{"id":"tLcRu7x-L-HP"}},{"cell_type":"code","source":["anomaly_train = pd.read_csv('/content/drive/data/anomaly_train.csv')\n","\n","groups = anomaly_train.groupby('anomaly_total_number')\n","data_to_plot = [group['anomaly_accumulation_hour'] for name, group in groups]\n","\n","plt.boxplot(data_to_plot, labels=anomaly_train['anomaly_total_number'].unique())\n","plt.title(\"Anomaly Accumulation Hour by Anomaly Total Number\")\n","plt.xlabel(\"Anomaly Total Number\")\n","plt.ylabel(\"Hour\")\n","plt.show()"],"metadata":{"id":"QAscS32hMBMr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def box_plot(group_name):\n","    # group_name = \"1B0\"\n","    filepath = f\"group_{group_name}.csv\"\n","    df = pd.read_csv(f\"/content/drive/日期刪除_分爐資料/{filepath}\")\n","    groups = df.groupby('anomaly_total_number')\n","    data_to_plot = [group['anomaly_accumulation_hour'] for name, group in groups]\n","\n","    plt.boxplot(data_to_plot, labels=df['anomaly_total_number'].unique())\n","    plt.title(f\"{group_name} - Accumulation Hour by Anomaly Total Number\")\n","    plt.xlabel(\"Anomaly Total Number\")\n","    plt.ylabel(\"Hour\")\n","\n","    plt.ylim(0, 10000)\n","\n","    plt.show()"],"metadata":{"id":"SODzPlzpM8ix"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for group_name in set(anomaly_train[\"oven_id\"]):\n","    box_plot(group_name)"],"metadata":{"id":"ZsW-yFxPNbqj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def plot(group_name):\n","    filepath = f\"group_{group_name}.csv\"\n","    df = pd.read_csv(f\"/content/drive/日期刪除_分爐資料/{filepath}\")\n","    # 計算每個 layer_id 對應的累積故障數量\n","\n","    cumulative_counts = df.groupby('layer_id')['anomaly_total_number'].sum()\n","\n","    # 畫圖\n","\n","    plt.bar(df['layer_id'].unique(), cumulative_counts, align='center', alpha=0.7)\n","    plt.xlabel(\"Layer ID\")\n","    plt.ylabel(\"Cumulative Anomaly Count\")\n","    plt.title(f\"{group_name} - Cumulative Anomaly Count by Layer ID\")\n","    plt.xticks(df['layer_id'].unique())\n","    plt.show()"],"metadata":{"id":"xkJzNxbFNeSs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for group_name in set(anomaly_train[\"oven_id\"]):\n","    plot(group_name)"],"metadata":{"id":"Si59n7-SNgqc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 步驟\n","\n","1. anomaly_train.csv 刪除日期不在範圍內的資料\n","2. 按照oven_id將資料分成10個csv檔\n","3. 將anomaly_train分爐後的資料跟cooler.csv、power.csv做結合\n","\n","### **1. 找日期不符合數據說明的數據**\n","\n","    1-1. 先把資料按照產線分兩組  \n","    1-2. 資料清洗，方法有兩種，如下：\n","        - 方法一：刪除日期不在範圍內的資料\n","        - 方法二：按照月份推測正確的年份（最終選擇方法一，所以底下沒有方法二的code）"],"metadata":{"id":"ZwbF51IlCDBF"}},{"cell_type":"code","source":["## 獲取當前工作路徑\n","current_directory = os.getcwd()"],"metadata":{"id":"oJgUtRQUIkcQ","executionInfo":{"status":"ok","timestamp":1694503450665,"user_tz":-480,"elapsed":359,"user":{"displayName":"林耘熙","userId":"06114259024859577512"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"c62a8792-6a9b-4210-ec8f-5341906aeda8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yniPbV6DBx73","colab":{"base_uri":"https://localhost:8080/","height":409},"executionInfo":{"status":"error","timestamp":1694503379154,"user_tz":-480,"elapsed":8,"user":{"displayName":"林耘熙","userId":"06114259024859577512"}},"outputId":"c617b915-92e9-4e98-e709-78c3d2998345"},"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-10-0928de526a51>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0manomaly_train_relative_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_directory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'data/row_data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'anomaly_train.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0manomaly_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manomaly_train_relative_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m## 1-1 分組\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mfilter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0manomaly_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"oven_id\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    329\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfind_stack_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 )\n\u001b[0;32m--> 331\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;31m# error: \"Callable[[VarArg(Any), KwArg(Any)], Any]\" has no\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 605\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1442\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1444\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1735\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1736\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1737\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    854\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 856\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    857\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/data/row_data/anomaly_train.csv'"]}],"source":["anomaly_train_relative_path = os.path.join(current_directory, 'data/row_data', 'anomaly_train.csv')\n","anomaly_train = pd.read_csv(anomaly_train_relative_path)\n","\n","## 1-1 分組\n","filter = anomaly_train[\"oven_id\"].str.startswith(\"1\")\n","\n","df_p1 = anomaly_train[filter]\n","df_p2 = anomaly_train[-filter]\n","\n","df_p1[\"date\"] = pd.to_datetime(df_p1[\"date\"])\n","df_p2[\"date\"] = pd.to_datetime(df_p2[\"date\"])\n","\n","## 1-2 刪資料\n","# 產線一日期範圍\n","start_date = pd.to_datetime(\"2021/12/27\")\n","end_date = pd.to_datetime(\"2022/9/1\")\n","\n","# 删除不在日期範圍內的row\n","df_p1_dataDelete = df_p1[(df_p1[\"date\"] >= start_date) & (df_p1[\"date\"] <= end_date)]\n","\n","# 產線二日期範圍\n","start_date = pd.to_datetime(\"2021/12/27\")\n","end_date = pd.to_datetime(\"2023/2/6\")\n","\n","# 删除不在日期範圍內的row\n","df_p2_dataDelete = df_p2[(df_p2[\"date\"] >= start_date) & (df_p2[\"date\"] <= end_date)]"]},{"cell_type":"markdown","source":["### **2. 按照oven_id將資料分成10個csv檔**"],"metadata":{"id":"ie7inXhSFVLA"}},{"cell_type":"code","source":["def devide(df, column_name, output_filepath):\n","    grouped = df.groupby(column_name)\n","\n","    for group_name, group_data in grouped:\n","        # 文件名\n","        csv_filename = f\"{group_name}.csv\"\n","        output_path = os.path.join(current_directory, output_filepath, csv_filename)\n","        print(output_path)\n","        # 輸出\n","        group_data.to_csv(output_path, index=False)\n","        print(f\"CSV 文件 {csv_filename} 已創建\")"],"metadata":{"id":"w9OQwsHAEHQy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["devide(df_p1_dataDelete, \"oven_id\", r\"data/anomaly_divided_data\")\n","devide(df_p2_dataDelete, \"oven_id\", r\"data/anomaly_divided_data\")"],"metadata":{"id":"ysjqs9nyFflk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **3. 結合cooler.csv、power.csv 資料**\n"],"metadata":{"id":"8JhrJTblM7nU"}},{"cell_type":"code","source":["oven_id_lst = ['1B0', '1C0', '1D0', '1E0', '1G0', '2B0', '2C0', '2D0', '2E0', '2G0']"],"metadata":{"id":"FRJgmjHvNXJx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["cooler_relative_path = os.path.join(current_directory, 'data/row_data', 'cooler.csv')\n","cooler = pd.read_csv(cooler_relative_path)\n","# '/content/drive/Shareddrives/MY @AG 數據競賽/data/cooler.csv'\n","\n","# 將 cooler_id 中以 \"temperature\" 結尾的行選擇出來，形成 temperature_df\n","temperature_df = cooler[cooler['cooler_id'].str.endswith('temperature')]\n","\n","# 將 cooler_id 中以 \"water_volume\" 結尾的行選擇出來，形成 water_volume_df\n","water_volume_df = cooler[cooler['cooler_id'].str.endswith('water_volume')]\n","\n","# 重新設定索引\n","temperature_df.reset_index(drop=True, inplace=True)\n","water_volume_df.reset_index(drop=True, inplace=True)\n","\n","water_volume_df = water_volume_df.set_index('cooler_id')\n","temperature_df = temperature_df.set_index('cooler_id')"],"metadata":{"id":"wXn8bCWrM8DJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["power_relative_path = os.path.join(current_directory, 'data/row_data', 'power.csv')\n","power = pd.read_csv(power_relative_path)\n","# '/content/drive/data/power.csv'\n","\n","bins = [0]\n","for index, row in power.iterrows():\n","    bins.append(row[\"accumulation_hour\"].split('-')[1])\n","\n","label_1 = list(power[\"power_setup(other_lamp)\"])\n","label_2 = list(power[\"power_setup(lamp_1_2_60_61_62_63_121_122)\"])\n","\n","spe_lamp = [1, 2, 60, 61, 62, 63, 121, 122]\n","spe_lamp = [str(i) for i in spe_lamp]"],"metadata":{"id":"ZzN2hZ2YVaCT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def combine_data(oven_id):\n","    input_file_path = os.path.join(current_directory, \"data/anomaly_divided_data\", f\"{oven_id}.csv\")\n","    df = pd.read_csv(input_file_path)\n","\n","    if oven_id == '1E0' or oven_id == '2E0':\n","        df['oven_id'] = oven_id\n","\n","    water_volumn = []\n","    temp_a = []\n","    temp_b = []\n","    for index, row in df.iterrows():\n","        id = int(row['layer_id'])\n","        volumn = (water_volume_df.loc[f'Slot{id}_water_volume'][oven_id] + water_volume_df.loc[f'Slot{id+1}_water_volume'][oven_id])/2\n","        water_volumn.append(volumn)\n","        a_temp = 0\n","        b_temp = 0\n","        if id == 19:\n","            a_temp = temperature_df.loc[f'S{id:02d}_A_temperature'][oven_id]\n","            b_temp = temperature_df.loc[f'S{id:02d}_B_temperature'][oven_id]\n","\n","        else:\n","            a_temp = (temperature_df.loc[f'S{id:02d}_A_temperature'][oven_id] + temperature_df.loc[f'S{id+1:02d}_A_temperature'][oven_id])/2\n","            b_temp = (temperature_df.loc[f'S{id:02d}_B_temperature'][oven_id] + temperature_df.loc[f'S{id+1:02d}_B_temperature'][oven_id])/2\n","        temp_a.append(a_temp)\n","        temp_b.append(b_temp)\n","    dic = pd.DataFrame({\n","        'water_volumn': water_volumn,\n","        'Temperature_A': temp_a,\n","        'Temperature_B': temp_b\n","    })\n","\n","    df = pd.concat([df, dic], axis=1)\n","\n","    ## power ##\n","    has_special_or_not = []\n","    for index, row in df.iterrows():\n","        has_special_or_not.append(0)\n","        for i in spe_lamp:\n","            if i in row[\"lamp_id\"].split('_'):\n","                has_special_or_not[index] += 1\n","                break\n","        has_special_or_not[index] /= int(row[\"anomaly_total_number\"])\n","\n","    has_special_or_not = pd.DataFrame({'lamp_special_rartio':has_special_or_not})\n","    df = pd.concat([df, has_special_or_not], axis = 1)\n","\n","    df[\"power(other)\"] = pd.cut(df[\"anomaly_accumulation_hour\"].values, bins=bins, labels=label_1)\n","    df[\"power(lamp_1_2_60_61_62_63_121_122)\"] = pd.cut(df[\"anomaly_accumulation_hour\"].values, bins=bins, labels=label_2)\n","\n","    power_count = []\n","    for index, row in df.iterrows():\n","        temp = (1-row[\"lamp_special_rartio\"])*row[\"power(other)\"] + row[\"lamp_special_rartio\"]*row[\"power(lamp_1_2_60_61_62_63_121_122)\"]\n","        power_count.append(temp)\n","\n","    power_count = pd.DataFrame({'power_count': power_count})\n","    df = pd.concat([df, power_count], axis=1)\n","\n","    column_to_move = 'anomaly_total_number'\n","    df[column_to_move] = df.pop(column_to_move)\n","\n","    output_path = os.path.join(current_directory, 'data/anomaly_with_power_cooler', f'{oven_id}.csv')\n","    # df.to_csv(f\"/content/drive/異常資料_power_cooler/{oven_id}.csv\")\n","    print(oven_id, 'finished')"],"metadata":{"id":"OPHpFmRKNace"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for id in oven_id_lst:\n","    combine_data(id)"],"metadata":{"id":"rcMT-FPhNckq"},"execution_count":null,"outputs":[]}]}